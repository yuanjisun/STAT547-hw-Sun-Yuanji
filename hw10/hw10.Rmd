---
title: "Homework 10"
author: "Yuanji Sun"
date: "December 6, 2017"
output: 
  html_document: 
    keep_md: yes
---
# Report process
I ran into and solved the following problems when doing this homework.

1. API key must be put in double quotation mark. Otherwise, the function will not work.
2. The raw weather data of different cities has different lengths, and even for the same city, the data length will change when the data is updated every a few hours. Thus, I selected the most important and common vaiables and created a dataframe.
3. For Part 3, I noticed that the missing value was in `nd` on the website. In order to generate an empty figure if all values are missing, I should use `na.strings = "nd"` and then convert it to numeric form. This is because I cannot use `NA_integer_` or other similar code in `read.table()`.
4. There is a bug of `ggplot()`. In the for loop, I used `ggplot(data=data, mapping=colnames(data)[j], y="depth"))`. Yes, it is right logically but when `j` changes, the previous figure will also be changed. This is because even though the figure is generated and saved in `plot_list`, the figure is still linked with `j` and will change. Thus, in my first trial, all figures were the same except the axis names.

Overall, this homework is finished smoothly. Many problems occurred in Part 3 but they were solved finally. This work will benefit me a lot in doing my research based on big data set in Oceanography. In the future, I only need to change the top-level URL of the database and then all sub-datapage can be analyzed without downloading any files.

__Please be patient when running codes in Part 3. Large dataset will be analyzed and it will take sometime.__

__Useful resource__

1. Google
2. Lecture notes
3. R for Data Science


# Load libraries
```{r}
library(tidyverse)
library(httr)
library(magrittr)
library(purrr)
library(glue)
library(stringr)
library(rvest)
library(xml2)
library(ggplot2)
library(cowplot)
library(knitr)
library(leaflet)
```

# Part 1: Work with OpenWeatherMap
Step 1: Write a function to get the weather data, and select important information for output.

__Remember to put your API Key in double quotation marks.__
```{r}
get_weather <- function(city_name, country_code, API_key) {
	query_string <- glue("api.openweathermap.org/data/2.5/weather?q={city_name},{country_code}&APPID={API_key}")
	weather_result <- GET(query_string)
	weather_content <- data.frame(content(weather_result))
	weather_output <- weather_content %>%
	        select(name, sys.country, coord.lon, coord.lat, weather.description, main.temp)
	return(weather_output)
}
```

Step 2: Get weather information of major cities in China. To get your own API key, please visit [Open Weather Map](https://openweathermap.org/api). A free API is enough for this homework.
```{r}
result_Beijing <- get_weather("Beijing","CN","7ce93051a99b9283279834f10c83f037")
result_Hangzhou <- get_weather("Hangzhou","CN","7ce93051a99b9283279834f10c83f037")
result_Shanghai <- get_weather("Shanghai","CN","7ce93051a99b9283279834f10c83f037")
result_Xiamen <- get_weather("Xiamen","CN","7ce93051a99b9283279834f10c83f037")
result_Chengdu <- get_weather("Chengdu","CN","7ce93051a99b9283279834f10c83f037")
result_Harbin <- get_weather("Harbin","CN","7ce93051a99b9283279834f10c83f037")
result_Tianjin <- get_weather("Tianjin","CN","7ce93051a99b9283279834f10c83f037")
result_Shijiazhuang <- get_weather("Shijiazhuang","CN","7ce93051a99b9283279834f10c83f037")
result_Xian <- get_weather("Xian","CN","7ce93051a99b9283279834f10c83f037")
result_Jinan <- get_weather("Jinan","CN","7ce93051a99b9283279834f10c83f037")
result_Zhenzhou <- get_weather("Zhenzhou","CN","7ce93051a99b9283279834f10c83f037")
result_Wuhan <- get_weather("Wuhan","CN","7ce93051a99b9283279834f10c83f037")
result_Guiyang <- get_weather("Guiyang","CN","7ce93051a99b9283279834f10c83f037")
result_Changsha <- get_weather("Changsha","CN","7ce93051a99b9283279834f10c83f037")
result_Lanzhou <- get_weather("Lanzhou","CN","7ce93051a99b9283279834f10c83f037")
result_Kunming <- get_weather("Kunming","CN","7ce93051a99b9283279834f10c83f037")
result_Nanning <- get_weather("Nanning","CN","7ce93051a99b9283279834f10c83f037")
result_Guangzhou <- get_weather("Guangzhou","CN","7ce93051a99b9283279834f10c83f037")
result_Shenyang <- get_weather("Shenyang","CN","7ce93051a99b9283279834f10c83f037")
result_Changchun <- get_weather("Changchun","CN","7ce93051a99b9283279834f10c83f037")
result_Nanchang <- get_weather("Nanchang","CN","7ce93051a99b9283279834f10c83f037")
result_df <- rbind(result_Beijing, result_Hangzhou, result_Shanghai, result_Xiamen, result_Chengdu,
		   result_Harbin, result_Tianjin, result_Shijiazhuang, result_Xian, result_Jinan,
		   result_Zhenzhou, result_Guiyang, result_Wuhan, result_Changsha, result_Lanzhou, 
		   result_Kunming, result_Nanning, result_Guangzhou, result_Shenyang, 
		   result_Changchun, result_Nanchang)
colnames(result_df) <- c("City", "Country", "longitude", "latitude", "Weather", "Temperature")
result_df
```

Step 3: Make an interactive map using `leaflet()` and add weather information on the pop-up.

__To view this map, please see HTML version. Map is NOT supported in MD file.__

```{r}
result_df %>%
        leaflet() %>%   
        addProviderTiles(providers$Esri.NatGeoWorldMap) %>%  
        addMarkers(popup = ~ Weather)
```

Step 4: Modify the function with a for loop.

It is a waste of time to get data one by one. I would like to enter all cities once and call the function only once. __This work is only designed for Chinese cities with a weather map (single country), so the country code is only "CN". If you want to use this for more than one countries, the function needs to be modified slightly.__
```{r}
get_weather_2 <- function(city_name, country_code, API_key) {
	all_result <- data.frame(matrix(ncol = 6, nrow = 0))
	col_name <- c("name", "sys.country", "coord.lon", "coord.lat", "weather.description", "main.temp")
	colnames(all_result) <- col_name
        for (i in 1:length(city_name)){
                query_string<-glue("api.openweathermap.org/data/2.5/weather?q={city_name[i]},{country_code}&APPID={API_key}")
                weather_result <- GET(query_string)
                weather_content <- data.frame(content(weather_result))
                weather_output <- weather_content %>%
	                select(name, sys.country, coord.lon, coord.lat, weather.description, main.temp)
                all_result <- rbind(all_result, weather_output)
        }
	return(all_result)
}
```

Let's have a try, getting a full results and an interactive map.
```{r}
city <- c("Beijing","Hangzhou","Shanghai","Xiamen","Chengdu","Harbin","Tianjin","Shijiazhuang","Xian",
	  "Jinan","Zhenzhou","Wuhan","Guiyang","Changsha","Lanzhou","Kunming","Nanning","Guangzhou",
	  "Shenyang","Changchun","Nanchang")
weather_all_city <- get_weather_2(city,"CN","7ce93051a99b9283279834f10c83f037")
colnames(result_df) <- c("City", "Country", "longitude", "latitude", "Weather", "Temperature")
result_df %>%
        leaflet() %>%   
        addProviderTiles(providers$Esri.NatGeoWorldMap) %>%  
        addMarkers(popup = ~ Weather)
```

# Part 2: Analyze oceanographic data (JGOFS) from single webpage
Step 1: Get the URL of the data set and store the URL contents to a variable.
```{r}
url <- "http://usjgofs.whoi.edu/jg/serv/jgofs/arabian/ttn-039/bottle.flat1?event%20eq%20092111.0"
page_title <- read_html(url)
```

Step 2: Extract the data in the text form.
```{r}
txt_data <- xml_text(page_title)
```

Step 3: Convert the text data into a dataframe.
```{r}
data <- read.table(text=txt_data,skip=1,col.names=c('event','sta', "cast", "date", "time", "lat_begin",
	"lon_begin", "bot", "press","temp", "sal_bot","O2_ml_L", "O2_umol_kg", "O2_umol_L", "O2_4", "NO3",
	"PO4", "SiO4", "NO2", "NH4"))
data$depth <- -data$press
```

Step 4: Generate figures for selected variable vs `press`. In Oceanography, `press` can be taken as depth.
```{r}
plot_list = list()
for (i in 16:18) {
        variable <- colnames(data)[i]
        plot_list[[i-15]] <- ggplot(data=data, mapping=aes_string(x=variable, y="depth")) +
        	geom_point() +
        	geom_path() +
                labs(x = colnames(data)[i], y = "Depth (m)")
}
print(plot_list)
```

# Part 3: Analyze oceanographic data (JGOFS) automatically from multiple webpage
Step 1: Get the URL of all data set, which contains lots of links of detailed data (just like the data in Part 1).
```{r}
main_URL <- "http://usjgofs.whoi.edu/jg/serv/jgofs/arabian/ttn-039/bottle.html0%7Bdir=usjgofs.whoi.edu/jg/dir/jgofs/arabian/ttn-039/,info=usjgofs.whoi.edu/jg/info/jgofs/arabian/ttn-039/bottle%7D"
```

Step 2: Extract event numbers in the webpage. All data in the dataframe are stored in character form.
```{r}
main_page_title <- read_html(main_URL)
raw_info <- main_page_title %>% xml_text()
info <- read.table(text=raw_info,skip=13,col.names=c("event", "sta", "cast", "date", "time", "lat_begin",
                                                        "lon_begin"), colClasses = "character")
event <- info$event
```

Step 3: Modify the event number by adding `20` before the number. This is because the link of the data ending with `20` + event numbers.
```{r}
for (i in 1:length(event)){
      event[i] <- paste("20", event[i], sep = "")
}
```

Step 4: Generating URLs for all data webpage. The URL has the commmon format, "http://usjgofs.whoi.edu/jg/serv/jgofs/arabian/ttn-039/bottle.flat1?event%20eq%" + event number. Let's have a look at all websites and check some of them.
```{r}
sub_URL = ""
for (i in 1:length(event)){
      sub_URL[i] <- paste("http://usjgofs.whoi.edu/jg/serv/jgofs/arabian/ttn-039/bottle.flat1?event%20eq%",
		event[i], sep = "")
}
sub_URL
```

Since the data in the second link is extremely bad (even worse than all `NA`), I would like to remove it.
```{r}
sub_URL <- sub_URL[-2]
```

Step 5. Analyze all data from different webpages at the same time. The idea is to put what I did in Part 1 in a loop and generate figures automatically.

__If all data in the data set is "NA", an empty plot will be returned. Also, if there are too many `NA` values (only a few valid data points), there will be no line in the figure.__
```{r}
k <- 0
plot_list = list()
for  (i in 1:length(sub_URL)){
        page_title <- read_html(sub_URL[i])
        txt_data <- xml_text(page_title)
        data <- read.table(text=txt_data,skip=1,col.names=c('event','sta', "cast", "date", "time", "lat_begin",
		"lon_begin","bot", "press","temp", "sal_bot","O2_ml_L", "O2_umol_kg","O2_umol_L", "O2_4", "NO3",
		"PO4", "SiO4", "NO2", "NH4"), na.strings = "nd")
        data$depth <- -data$press
        for (j in 16:18) {
        	k <- k+1
                variable <- colnames(data)[j]
                data[[variable]] <- as.numeric(data[[variable]])
                plot_list[[k]] <- ggplot(data=data, mapping=aes_string(x=variable, y="depth")) +
                        geom_point() +
                        geom_path() +
                        labs(x = colnames(data)[j], y = "Depth (m)", title = paste("Event", event[i],
                        	sep = " "))
        }
}
print(plot_list)

# Alternative way to print all plots (more complex way)
# plot_list <- c(plot_list, nrow=1)
# print(do.call(plot_grid, plot_list))
```


[This is the end of Homework 10.]
