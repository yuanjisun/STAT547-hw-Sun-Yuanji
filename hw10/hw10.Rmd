---
title: "Homework 10"
author: "Yuanji Sun"
date: "December 6, 2017"
output: 
  html_document: 
    keep_md: yes
---

# Load libraries
```{r}
library(tidyverse)
library(httr)
library(magrittr)
library(purrr)
library(glue)
library(stringr)
library(rvest)
library(xml2)
library(ggplot2)
library(cowplot)
library(leaflet)
```

# Part 1: Work with OpenWeatherMap
Step 1: Write a function to get the weather data, and select important information for output.

__Remember to put your API Key in double quotation marks.__
```{r}
get_weather <- function(city_name, country_code, API_key) {
	query_string <- glue("api.openweathermap.org/data/2.5/weather?q={city_name},{country_code}&APPID={API_key}")
	weather_result <- GET(query_string)
	weather_content <- data.frame(content(weather_result))
	weather_output <- weather_content %>%
	        select(name, sys.country, coord.lon, coord.lat, weather.description, main.temp)
	return(weather_output)
}
```

Step 2: Get weather information of major cities in China.
```{r}
result_Beijing <- get_weather("Beijing","CN","7ce93051a99b9283279834f10c83f037")
result_Hangzhou <- get_weather("Hangzhou","CN","7ce93051a99b9283279834f10c83f037")
result_Shanghai <- get_weather("Shanghai","CN","7ce93051a99b9283279834f10c83f037")
result_Xiamen <- get_weather("Xiamen","CN","7ce93051a99b9283279834f10c83f037")
result_Chengdu <- get_weather("Chengdu","CN","7ce93051a99b9283279834f10c83f037")
result_Harbin <- get_weather("Harbin","CN","7ce93051a99b9283279834f10c83f037")
result_Tianjin <- get_weather("Tianjin","CN","7ce93051a99b9283279834f10c83f037")
result_Shijiazhuang <- get_weather("Shijiazhuang","CN","7ce93051a99b9283279834f10c83f037")
result_Xian <- get_weather("Xian","CN","7ce93051a99b9283279834f10c83f037")
result_Jinan <- get_weather("Jinan","CN","7ce93051a99b9283279834f10c83f037")
result_Zhenzhou <- get_weather("Zhenzhou","CN","7ce93051a99b9283279834f10c83f037")
result_Wuhan <- get_weather("Wuhan","CN","7ce93051a99b9283279834f10c83f037")
result_Guiyang <- get_weather("Guiyang","CN","7ce93051a99b9283279834f10c83f037")
result_Changsha <- get_weather("Changsha","CN","7ce93051a99b9283279834f10c83f037")
result_Lanzhou <- get_weather("Lanzhou","CN","7ce93051a99b9283279834f10c83f037")
result_Kunming <- get_weather("Kunming","CN","7ce93051a99b9283279834f10c83f037")
result_Nanning <- get_weather("Nanning","CN","7ce93051a99b9283279834f10c83f037")
result_Guangzhou <- get_weather("Guangzhou","CN","7ce93051a99b9283279834f10c83f037")
result_Shenyang <- get_weather("Shenyang","CN","7ce93051a99b9283279834f10c83f037")
result_Changchun <- get_weather("Changchun","CN","7ce93051a99b9283279834f10c83f037")
result_Nanchang <- get_weather("Nanchang","CN","7ce93051a99b9283279834f10c83f037")
(result_df <- rbind(result_Beijing, result_Hangzhou, result_Shanghai, result_Xiamen, result_Chengdu, result_Harbin, 
                    result_Tianjin, result_Shijiazhuang, result_Xian, result_Jinan, result_Zhenzhou, result_Guiyang,
                    result_Wuhan, result_Changsha, result_Lanzhou, result_Kunming, result_Nanning, result_Guangzhou,
                    result_Shenyang, result_Changchun, result_Nanchang))

```

Step 3: Make an interactive map using `leaflet()` and add weather information on the pop-up. (Sorry, city names in the map are Chinese.)

__To view this map, please see HTML version. Map is NOT supported in MD file.__
```{r}
leaflet() %>%
        addTiles() %>%
        addMarkers(lng=result_df$coord.lon, lat=result_df$coord.lat, popup=result_df$weather.description)
```

# Part 2: Analyze oceanographic data (JGOFS) from single webpage
Step 1: Get the URL of the data set and store the URL contents to a variable.
```{r}
url <- "http://usjgofs.whoi.edu/jg/serv/jgofs/arabian/ttn-039/bottle.flat1?event%20eq%20092111.0"
page_title <- read_html(url)
```

Step 2: Extract the data in the text form.
```{r}
txt_data <- xml_text(page_title)
```

Step 3: Convert the text data into a dataframe.
```{r}
data <- read.table(text=txt_data,skip=1,col.names=c('event','sta', "cast", "date", "time", "lat_begin", "lon_begin",
                                                    "bot", "press","temp", "sal_bot","O2_ml_L", "O2_umol_kg",
                                                    "O2_umol_L", "O2_4", "NO3", "PO4", "SiO4", "NO2", "NH4"))
data$depth <- -data$press
data
```

Step 4: Generate figures for selected variable vs `press`. `press` can be taken as depth.
```{r}
plot_list = list()
for (i in 16:20) {
        variable <- colnames(data)[i]
        plot_list[[i-15]] <- ggplot(data=data, mapping=aes_string(x=variable, y="depth")) +
        	geom_point() +
        	geom_path() +
                labs(x = colnames(data)[i], y = "Depth (m)")
}
print(plot_list)
```

# Part 3: Analyze oceanographic data (JGOFS) automatically from multiple webpage
Step 1: Get the URL of all data set, which contains lots of links of detailed data (just like the data in Part 1).
```{r}
main_URL <- "http://usjgofs.whoi.edu/jg/serv/jgofs/arabian/ttn-039/bottle.html0%7Bdir=usjgofs.whoi.edu/jg/dir/jgofs/arabian/ttn-039/,info=usjgofs.whoi.edu/jg/info/jgofs/arabian/ttn-039/bottle%7D"
```

Step 2: Extract event numbers in the webpage. All data in the dataframe are stored in character form.
```{r}
main_page_title <- read_html(main_URL)
raw_info <- main_page_title %>% xml_text()
info <- read.table(text=raw_info,skip=13,col.names=c("event", "sta", "cast", "date", "time", "lat_begin",
                                                        "lon_begin"), colClasses = "character")
event <- info$event
```

Step 3: Modify the event number by adding `20` before the number. This is because the link of the data ending with `20` + event numbers.
```{r}
for (i in 1:length(event)){
      event[i] <- paste("20", event[i], sep = "")
}
```

Step 4: Generating URLs for all data webpage. The URL has the commmon format, "http://usjgofs.whoi.edu/jg/serv/jgofs/arabian/ttn-039/bottle.flat1?event%20eq%" + event number. Let's have a look at all websites and check some of them.
```{r}
sub_URL = ""
for (i in 1:length(event)){
      sub_URL[i] <- paste("http://usjgofs.whoi.edu/jg/serv/jgofs/arabian/ttn-039/bottle.flat1?event%20eq%",
                          event[i], sep = "")
}
sub_URL
```

Step 5. Analyze all data from different webpages at the same time. The idea is to put what I did in Part 1 in a loop and generate figures automatically.

__If all data in the data set is "NA", an empty plot will be returned. Also, if there are too many `NA` values (only a few valid data points), there will be no line in the figure.__
```{r}
k <- 0
plot_list = list()
for  (i in 1:length(sub_URL)){
        page_title <- read_html(sub_URL[i])
        txt_data <- xml_text(page_title)
        data <- read.table(text=txt_data,skip=1,col.names=c('event','sta', "cast", "date", "time", "lat_begin",
                                                            "lon_begin","bot", "press","temp", "sal_bot","O2_ml_L",
                                                            "O2_umol_kg","O2_umol_L", "O2_4", "NO3", "PO4", "SiO4",
                                                            "NO2", "NH4"), na.strings = "nd")
        data$depth <- -data$press
        for (j in 16:20) {
        	k <- k+1
                variable <- colnames(data)[j]
                data[[variable]] <- as.numeric(data[[variable]])
                plot_list[[k]] <- ggplot(data=data, mapping=aes_string(x=variable, y="depth")) +
                        geom_point() +
                        geom_path() +
                        labs(x = colnames(data)[j], y = "Depth (m)", title = paste("Event", event[i], sep = " "))
        }
}
print(plot_list)

# Alternative way to print all plots (more complex way)
# plot_list <- c(plot_list, nrow=1)
# print(do.call(plot_grid, plot_list))
```


[This is the end of Homework 10.]
